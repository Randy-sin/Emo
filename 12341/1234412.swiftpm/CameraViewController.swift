@preconcurrency import Vision
import UIKit
import AVFoundation

/// ç®¡ç†æ‘„åƒå¤´ä¼šè¯å¹¶æ˜¾ç¤ºé¢„è§ˆ
class CameraViewController: UIViewController, AVCaptureVideoDataOutputSampleBufferDelegate {
    private var captureSession: AVCaptureSession!
    private var videoOutput: AVCaptureVideoDataOutput!
    // æ·»åŠ ä¸“é—¨çš„è§†è§‰å¤„ç†é˜Ÿåˆ—
    private let visionQueue = DispatchQueue(label: "com.vision.queue")

    override func viewDidLoad() {
        super.viewDidLoad()
        setupCamera() // åˆå§‹åŒ–æ‘„åƒå¤´
    }

    override var supportedInterfaceOrientations: UIInterfaceOrientationMask {
        // æ”¯æŒæ‰€æœ‰æ–¹å‘ï¼Œä½†åœ¨ viewWillAppear ä¸­è®¾ç½®ä¸ºæ¨ªå±
        return .all
    }
    
    override var shouldAutorotate: Bool {
        return true
    }
    
    override var preferredInterfaceOrientationForPresentation: UIInterfaceOrientation {
        return .landscapeRight
    }

    private func setupCamera() {
        // åˆ›å»ºæ•è·ä¼šè¯
        captureSession = AVCaptureSession()
        captureSession.sessionPreset = .high

        // è·å–å‰ç½®æ‘„åƒå¤´
        guard let videoDevice = AVCaptureDevice.default(.builtInWideAngleCamera, for: .video, position: .front) else {
            print("Unable to access camera")
            return
        }

        do {
            // è®¾ç½®æ‘„åƒå¤´è¾“å…¥
            let videoInput = try AVCaptureDeviceInput(device: videoDevice)
            if captureSession.canAddInput(videoInput) {
                captureSession.addInput(videoInput)
            }

            // è®¾ç½®è§†é¢‘è¾“å‡º
            videoOutput = AVCaptureVideoDataOutput()
            videoOutput.setSampleBufferDelegate(self, queue: DispatchQueue(label: "videoQueue"))
            if captureSession.canAddOutput(videoOutput) {
                captureSession.addOutput(videoOutput)
            }

            // è®¾ç½®è§†é¢‘æ–¹å‘
            if let connection = videoOutput.connection(with: .video) {
                if connection.isVideoOrientationSupported {
                    connection.videoOrientation = .landscapeRight
                }
                if connection.isVideoMirroringSupported {
                    connection.isVideoMirrored = true
                }
            }

            // è®¾ç½®é¢„è§ˆå±‚
            let previewLayer = AVCaptureVideoPreviewLayer(session: captureSession)
            previewLayer.videoGravity = .resizeAspectFill
            
            // è°ƒæ•´é¢„è§ˆå±‚å¤§å°å’Œæ–¹å‘
            let bounds = view.bounds
            let previewFrame = CGRect(x: 0, y: 0, width: bounds.width, height: bounds.height)
            previewLayer.frame = previewFrame
            previewLayer.connection?.videoOrientation = .landscapeRight
            
            view.layer.addSublayer(previewLayer)

            // å¼€å§‹æ•è·ä¼šè¯
            captureSession.startRunning()
        } catch {
            print("Camera initialization failed: \(error)")
        }
    }

    override func viewWillAppear(_ animated: Bool) {
        super.viewWillAppear(animated)
        // è®¾ç½®ä¸ºæ¨ªå±
        let windowScene = view.window?.windowScene
        windowScene?.requestGeometryUpdate(.iOS(interfaceOrientations: .landscapeRight))
        setNeedsUpdateOfSupportedInterfaceOrientations()
    }

    // æ˜¾å¼æ ‡è®°æ­¤æ–¹æ³•ä¸ºééš”ç¦»çš„
    nonisolated func captureOutput(_ output: AVCaptureOutput, didOutput sampleBuffer: CMSampleBuffer, from connection: AVCaptureConnection) {
        guard let pixelBuffer = CMSampleBufferGetImageBuffer(sampleBuffer) else {
            print("âš ï¸ æ— æ³•è·å– pixelBuffer")
            return
        }
        
        // åœ¨é˜Ÿåˆ—ä¸­å¤„ç† Vision è¯·æ±‚ï¼Œä½¿ç”¨æ­£ç¡®çš„å›¾åƒæ–¹å‘
        let handler = VNImageRequestHandler(cvPixelBuffer: pixelBuffer, orientation: .right)
        let request = VNDetectFaceLandmarksRequest()
        
        visionQueue.async { [weak self] in
            do {
                try handler.perform([request])
                guard let observations = request.results else {
                    print("âš ï¸ æœªæ£€æµ‹åˆ°ä»»ä½•ç»“æœ")
                    return
                }
                print("âœ… æ£€æµ‹åˆ° \(observations.count) ä¸ªé¢éƒ¨")
                Task { @MainActor in
                    await self?.handleSmileDetection(observations)
                }
            } catch {
                print("âŒ Vision error: \(error)")
            }
        }
    }
    
    @MainActor
    private func handleSmileDetection(_ observations: [VNFaceObservation]) async {
        for observation in observations {
            if let landmarks = observation.landmarks {
                print("âœ… æ£€æµ‹åˆ°é¢éƒ¨ç‰¹å¾ç‚¹")
                if let mouth = landmarks.outerLips {
                    print("âœ… æ£€æµ‹åˆ°å˜´éƒ¨è½®å»“")
                    let mouthPoints = mouth.normalizedPoints
                    print("ğŸ‘„ å˜´éƒ¨ç‚¹ä½ï¼š\(mouthPoints)")
                    let isSmiling = await VisionProcessor.detectSmile(mouthPoints: mouthPoints)
                    print("ğŸ˜Š å¾®ç¬‘æ£€æµ‹ç»“æœï¼š\(isSmiling)")
                    NotificationCenter.default.post(
                        name: Notification.Name("SmileDetected"),
                        object: nil,
                        userInfo: ["isSmiling": isSmiling]
                    )
                } else {
                    print("âš ï¸ æœªæ£€æµ‹åˆ°å˜´éƒ¨è½®å»“")
                }
            } else {
                print("âš ï¸ æœªæ£€æµ‹åˆ°é¢éƒ¨ç‰¹å¾ç‚¹")
            }
        }
    }
}
